import os
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Global vectorizer to ensure consistent feature space
vectorizer = TfidfVectorizer()

# Function to read and analyze CSV files in a folder
def analyze_csv_files(folder_path):
    all_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
    pattern_list = []
    global vectorizer  # Use the global vectorizer to ensure consistent feature space
    
    for file_name in all_files:
        file_path = os.path.join(folder_path, file_name)
        df = pd.read_csv(file_path)
        pattern = extract_pattern_from_df(df)
        pattern_list.append((file_name, pattern))
    
    return pattern_list

# Function to extract pattern from DataFrame (example: using TF-IDF)
def extract_pattern_from_df(df):
    # Use the global vectorizer to ensure the same feature space
    df_str = df.astype(str).apply(lambda x: ' '.join(x), axis=1)
    tfidf_matrix = vectorizer.fit_transform(df_str)
    pattern = tfidf_matrix.mean(axis=0)
    pattern_array = np.asarray(pattern).reshape(1, -1)  # Convert to NumPy array with consistent shape
    return pattern_array

# Function to match patterns with file names in another folder
def match_patterns_with_filenames(pattern_list, target_folder):
    target_files = [f for f in os.listdir(target_folder) if f.endswith('.csv')]
    best_match = None
    best_similarity = -1
    
    for target_file in target_files:
        target_file_path = os.path.join(target_folder, target_file)
        target_df = pd.read_csv(target_file_path)
        target_pattern = extract_pattern_from_df(target_df)
        
        for (file_name, pattern) in pattern_list:
            # Ensure pattern arrays have consistent dimensions
            if pattern.shape[1] == target_pattern.shape[1]:
                # Calculate similarity (cosine similarity)
                similarity = cosine_similarity(pattern, target_pattern)
                
                # Track the best match
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = target_file
    
    return best_match

# Function to predict output file name for a new input file
def predict_output_file(new_file_path, pattern_list, target_folder):
    new_df = pd.read_csv(new_file_path)
    new_pattern = extract_pattern_from_df(new_df)
    
    # Match new pattern with file names in target folder
    matched_file = match_patterns_with_filenames([(new_file_path, new_pattern)], target_folder)
    return matched_file

# Example usage
if __name__ == "__main__":
    # Specify paths for your folders
    train_folder_path = '/content/sample_data'  # Change this to your train folder path
    match_folder_path = '/content/ICD'  # Change this to your match folder path
    new_file_path = '/content/Book2.csv'  # Change this to your new file path
    # Analyze CSV files in the train folder
    pattern_list = analyze_csv_files(train_folder_path)
    
    # Predict output file name for the new input file
    output_file_name = predict_output_file(new_file_path, pattern_list, match_folder_path)
    
    print(f"Predicted output file name: {output_file_name}"




import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Configuration: Define paths here
config = {
    'training_file_path': '/content/training_data.csv',  # Path to your training CSV
    'new_file_path': '/content/new_file.csv',  # Path to the new file
}

# Initialize the vectorizer
vectorizer = TfidfVectorizer(stop_words='english')

def load_training_data(training_file_path):
    df = pd.read_csv(training_file_path)
    
    # Ensure necessary columns exist
    if 'train_file_path' not in df.columns or 'match_file_path' not in df.columns:
        raise ValueError("CSV must contain 'train_file_path' and 'match_file_path' columns")

    # Extract file paths and corresponding match files
    train_file_paths = df['train_file_path'].dropna().unique()
    match_file_paths = df['match_file_path'].dropna().unique()
    
    if len(train_file_paths) != len(match_file_paths):
        raise ValueError("The number of training file paths must match the number of match file paths.")
    
    # Fit vectorizer on all training files
    all_texts = []
    for file_path in train_file_paths:
        if isinstance(file_path, str) and os.path.isfile(file_path):
            try:
                df = pd.read_csv(file_path)
                all_texts.extend(df.astype(str).apply(lambda x: ' '.join(x), axis=1))
            except Exception as e:
                print(f"Error reading file {file_path}: {e}")

    # Fit vectorizer on combined text data
    vectorizer.fit(all_texts)
    
    pattern_list = analyze_csv_files(train_file_paths)
    
    return pattern_list, match_file_paths

def analyze_csv_files(file_paths):
    pattern_list = []
    
    for file_path in file_paths:
        if isinstance(file_path, str) and os.path.isfile(file_path):
            try:
                df = pd.read_csv(file_path)
                pattern = extract_pattern_from_df(df)
                pattern_list.append((file_path, pattern))
            except Exception as e:
                print(f"Error processing file {file_path}: {e}")
        else:
            print(f"Invalid or missing file path: {file_path}")
    
    return pattern_list

def extract_pattern_from_df(df):
    df_str = df.astype(str).apply(lambda x: ' '.join(x), axis=1)
    tfidf_matrix = vectorizer.transform(df_str)  # Use transform instead of fit_transform
    pattern = tfidf_matrix.mean(axis=0)
    pattern_array = np.asarray(pattern).reshape(1, -1)  # Convert to NumPy array with consistent shape
    return pattern_array

def predict_file_name(new_file_path, pattern_list, match_file_paths):
    if isinstance(new_file_path, str) and os.path.isfile(new_file_path):
        try:
            new_df = pd.read_csv(new_file_path)
            new_pattern = extract_pattern_from_df(new_df)
            similarities = []
            
            for _, pattern in pattern_list:
                similarity = cosine_similarity(pattern, new_pattern)
                similarities.append(similarity)
            
            best_match_idx = np.argmax(similarities)
            return match_file_paths[best_match_idx]
        except Exception as e:
            print(f"Error predicting output for file {new_file_path}: {e}")
            return None
    else:
        print(f"Invalid or missing new file path: {new_file_path}")
        return None

# Main function
if __name__ == "__main__":
    # Load and analyze training data
    pattern_list, match_file_paths = load_training_data(config['training_file_path'])
    
    # Predict file name for the new file
    predicted_file_name = predict_file_name(config['new_file_path'], pattern_list, match_file_paths)
    print(f"Predicted file name: {predicted_file_name}")










import os
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Configuration: Define paths here
config = {
    'training_file_path': '/content/training_data.csv',  # Path to your training CSV
    'new_file_path': '/content/new_file.csv',  # Path to the new file
}

# Initialize the vectorizer
vectorizer = TfidfVectorizer(stop_words='english')

def load_training_data(training_file_path):
    df = pd.read_csv(training_file_path)
    
    # Ensure necessary columns exist
    if 'train_file_path' not in df.columns or 'match_file_path' not in df.columns:
        raise ValueError("CSV must contain 'train_file_path' and 'match_file_path' columns")

    # Extract file paths and corresponding match files
    train_file_paths = df['train_file_path'].dropna().unique()
    match_file_paths = df['match_file_path'].dropna().unique()
    
    if len(train_file_paths) != len(match_file_paths):
        raise ValueError("The number of training file paths must match the number of match file paths.")
    
    # Fit vectorizer on all training files
    all_texts = []
    for file_path in train_file_paths:
        if isinstance(file_path, str) and os.path.isfile(file_path):
            try:
                df = pd.read_csv(file_path)
                all_texts.extend(df.astype(str).apply(lambda x: ' '.join(x), axis=1))
            except Exception as e:
                print(f"Error reading file {file_path}: {e}")
        else:
            print(f"Invalid or missing file path: {file_path}")

    # Fit vectorizer on combined text data
    vectorizer.fit(all_texts)
    
    pattern_list = analyze_csv_files(train_file_paths)
    
    return pattern_list, match_file_paths

def analyze_csv_files(file_paths):
    pattern_list = []
    
    for file_path in file_paths:
        if isinstance(file_path, str) and os.path.isfile(file_path):
            try:
                df = pd.read_csv(file_path)
                pattern = extract_pattern_from_df(df)
                pattern_list.append((file_path, pattern))
            except Exception as e:
                print(f"Error processing file {file_path}: {e}")
        else:
            print(f"Invalid or missing file path: {file_path}")
    
    return pattern_list

def extract_pattern_from_df(df):
    df_str = df.astype(str).apply(lambda x: ' '.join(x), axis=1)
    tfidf_matrix = vectorizer.transform(df_str)  # Use transform instead of fit_transform
    pattern = tfidf_matrix.mean(axis=0)
    pattern_array = np.asarray(pattern).reshape(1, -1)  # Convert to NumPy array with consistent shape
    return pattern_array

def predict_file_name(new_file_path, pattern_list, match_file_paths):
    if isinstance(new_file_path, str) and os.path.isfile(new_file_path):
        try:
            new_df = pd.read_csv(new_file_path)
            new_pattern = extract_pattern_from_df(new_df)
            similarities = []
            
            for _, pattern in pattern_list:
                similarity = cosine_similarity(pattern, new_pattern)
                similarities.append(similarity)
            
            best_match_idx = np.argmax(similarities)
            return match_file_paths[best_match_idx]
        except Exception as e:
            print(f"Error predicting output for file {new_file_path}: {e}")
            return None
    else:
        print(f"Invalid or missing new file path: {new_file_path}")
        return None

# Main function
if __name__ == "__main__":
    # Load and analyze training data
    pattern_list, match_file_paths = load_training_data(config['training_file_path'])
    
    # Predict file name for the new file
    predicted_file_name = predict_file_name(config['new_file_path'], pattern_list, match_file_paths)
    print(f"Predicted file name: {predicted_file_name}")
